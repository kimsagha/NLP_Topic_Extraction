{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Project\n",
    "## Topic extraction\n",
    "The purpose of this project is to extract topics from news articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step-by-step Process\n",
    "1. Find a suitable NLP model to use for topic extraction: LDA\n",
    "2. Preprocess the data\n",
    "3. Get results\n",
    "4. Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "from gensim.parsing.preprocessing import strip_tags, strip_punctuation, strip_multiple_whitespaces, strip_numeric, remove_stopwords, strip_short, stem_text\n",
    "import gensim  # necessary?\n",
    "from gensim import corpora\n",
    "from gensim.models import Word2Vec, TfidfModel, LdaMulticore, CoherenceModel\n",
    "import spacy\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Large dataset model:\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # read in data\n",
    "# df_1 = pd.read_csv('Data/articles1.csv')['content'].to_frame()  # only get content-column\n",
    "# df_2 = pd.read_csv('Data/articles2.csv')['content'].to_frame()\n",
    "# df_3 = pd.read_csv('Data/articles3.csv')['content'].to_frame()\n",
    "# df = df_1.append(df_2).append(df_3)\n",
    "# print('\\nData set, shape:', df.shape)\n",
    "# print(df.head(5))\n",
    "\n",
    "# # check for missing data\n",
    "# print(df.isna().sum())  # shows no null values in content-column\n",
    "\n",
    "# # split data into ~67% training and ~33% testing\n",
    "# train, test = train_test_split(df, test_size=0.33, random_state=1)\n",
    "# print('\\nTraining data set, shape:', train.shape)\n",
    "# print('Testing data set, shape:', test.shape)\n",
    "\n",
    "# # reset indices\n",
    "# train = train.reset_index(drop=True)\n",
    "# test = test.reset_index(drop=True)\n",
    "# print(train.head(5))\n",
    "# print('\\n', test.head(5))\n",
    "\n",
    "# Small dataset model:\n",
    "# read in data\n",
    "df = pd.read_csv('Data/articles1.csv')['content'].to_frame()\n",
    "# df.drop(df.index[0:49900],0,inplace=True)  # drop a few rows to make dataset smaller and more manageable\n",
    "print('\\nData set, shape:', df.shape)\n",
    "# check for missing data\n",
    "print(df.isna().sum())  # shows no null values in content-column\n",
    "\n",
    "# pd.set_option('display.max_colwidth', None)\n",
    "print(df.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process the Data\n",
    "Normalisation, cleaning, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load spacy nlp pre-processing pipeline to use for lemmatization\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create filter for gensim nlp pre-processing pipeline to include all steps except stemmatization\n",
    "CUSTOM_FILTERS = [lambda x: x.lower(),  # lowercase\n",
    "                  strip_tags,\n",
    "                  strip_punctuation,  # replace punctuation with whitespace\n",
    "                  strip_multiple_whitespaces,  # remove repeating whitespaces\n",
    "                  strip_numeric,  # remove numbers\n",
    "                  remove_stopwords,  # remove stopwords\n",
    "                  strip_short,  # remove words with less than 3 characters\n",
    "                  #  stem_text  # return porter-stemmed text,\n",
    "                 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = \"Hello, my name is something you'll never guess, Kim! ...But I wrote my signature. Right! My parents called me this, what can I say?\"\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test sample string without filtered pipeline, i.e., with stemmatizer\n",
    "test_a = preprocess_string(sample)\n",
    "print(test_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test sample string with filtered pipeline and lemmatizer\n",
    "test_b = ' '.join(preprocess_string(sample, CUSTOM_FILTERS))  # pre-process without stemmatizing\n",
    "lem = [token.lemma_ for token in nlp(test_b)]  # lemmatize\n",
    "print(lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_articles(x):\n",
    "    prep = ' '.join(preprocess_string(x, CUSTOM_FILTERS))\n",
    "    return [token.lemma_ for token in nlp(prep)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply final pipeline to all data\n",
    "df['preprocessed'] = df['content'].apply(preprocess_articles)\n",
    "print(df['preprocessed'].head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare pre-processed articles for topic extraction models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create corpus: article contents embedded according to BoW-model using dictionary\n",
    "article_dict = corpora.Dictionary(df['preprocessed'])\n",
    "article_dict.filter_extremes(no_below=3)\n",
    "bow_corpus = [article_dict.doc2bow(line) for line in df['preprocessed']]\n",
    "tfidf = TfidfModel(bow_corpus)  # take term-frequency into account in bow-model, fit\n",
    "tfidf_corpus = tfidf[bow_corpus]  # transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA - Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123456)\n",
    "lda_model = LdaMulticore(corpus=tfidf_corpus,\n",
    "                         id2word=article_dict,\n",
    "                         random_state=100,\n",
    "                         num_topics=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "lda_model.save(\"LDA_model/LDA_model\")\n",
    "\n",
    "# load the model\n",
    "# lda_model = LdaModel.load(\"LDA_model/LDA_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keywords per topic\n",
    "lda_model.print_topics(-1)\n",
    "\n",
    "doc_lda = lda_model[tfidf_corpus]\n",
    "\n",
    "# print(lda_model.print_topic(0))\n",
    "# print('\\n', lda_model.print_topic(1))\n",
    "# print('\\n', lda_model.print_topic(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Coherence Score\n",
    "coherence_model = CoherenceModel(model=lda_model, texts=df['preprocessed'], dictionary=article_dict, coherence='c_v')\n",
    "coherence_score = coherence_model.get_coherence()\n",
    "print('Coherence Score: ', coherence_score)\n",
    "\n",
    "# change num of topics to increase coherence score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print topic(s) per article\n",
    "# for topic in doc_lda:\n",
    "#     print(topic)\n",
    "\n",
    "# doc_lda[4]\n",
    "\n",
    "# print dominant topic(s) of the 20 first articles\n",
    "for i in range(20):\n",
    "    print(doc_lda[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis = pyLDAvis.gensim_models.prepare(topic_model=lda_model, corpus=tfidf_corpus, dictionary=article_dict)\n",
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

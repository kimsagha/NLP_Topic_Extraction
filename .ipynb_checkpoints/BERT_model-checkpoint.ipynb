{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Project\n",
    "## Topic extraction\n",
    "The purpose of this project is to extract topics from news articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step-by-step Process\n",
    "1. Find a suitable NLP model to use for topic extraction: BERT\n",
    "2. Preprocess the data\n",
    "3. Get results\n",
    "4. Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# !pip install bertopic --ignore-installed llvmlite\n",
    "# !pip install bertopic[visualization] --ignore-installed llvmlite\n",
    "# !pip install tensorflow-gpu==1.15\n",
    "# !pip install -U bert-serving-server bert-serving-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "from gensim.parsing.preprocessing import strip_tags, strip_punctuation, strip_multiple_whitespaces, strip_numeric, remove_stopwords, strip_short, stem_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data set, shape: (50000, 1)\n",
      "                                             content\n",
      "0  WASHINGTON  —   Congressional Republicans have...\n"
     ]
    }
   ],
   "source": [
    "# read in data\n",
    "df = pd.read_csv('Data/articles1.csv')['content'].to_frame()\n",
    "print('\\nData set, shape:', df.shape)\n",
    "print(df.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# check for missing data\n",
    "print(df.isna().sum())  # shows no null values in content-column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pipeline\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create filter for gensim nlp pre-processing pipeline to include all steps except stemmatization\n",
    "CUSTOM_FILTERS = [lambda x: x.lower(),  # lowercase\n",
    "                  strip_tags,\n",
    "                  strip_punctuation,  # replace punctuation with whitespace\n",
    "                  strip_multiple_whitespaces,  # remove repeating whitespaces\n",
    "                  strip_numeric,  # remove numbers\n",
    "                  remove_stopwords,  # remove stopwords\n",
    "                  strip_short,  # remove words with less than 3 characters\n",
    "                  #  stem_text  # return porter-stemmed text,\n",
    "                 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_articles(x):\n",
    "    prep = ' '.join(preprocess_string(x, CUSTOM_FILTERS))\n",
    "    return [token.lemma_ for token in nlp(prep)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply final pipeline to all data\n",
    "df['preprocessed'] = df['content'].apply(preprocess_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [washington, congressional, republicans, new, ...\n",
      "Name: preprocessed, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# print head of preprocessed df\n",
    "print(df['preprocessed'].head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the df with added preprocessed column into a csv file\n",
    "df.to_csv('Data/articles_preprocessed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings with BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert-serving-start -model_dir /BERT/wwm_uncased_L-24_H-1024_A-16/ -num_worker=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_serving.client import BertClient()\n",
    "client = BertClient()\n",
    "vectors = client.encode([“dog”],[“cat”],[“man”])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

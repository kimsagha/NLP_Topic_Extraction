{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Project\n",
    "## Topic extraction\n",
    "The purpose of this project is to extract topics from news articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step-by-step Process\n",
    "1. Find a suitable NLP model to use for topic extraction: RAKE\n",
    "2. Preprocess the data\n",
    "3. Get results\n",
    "4. Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import pandas as pd\n",
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "from gensim.parsing.preprocessing import strip_tags, strip_punctuation, strip_multiple_whitespaces, strip_numeric, remove_stopwords, strip_short, stem_text\n",
    "import gensim  # necessary?\n",
    "from gensim import corpora\n",
    "import spacy\n",
    "from rake_nltk import Rake\n",
    "import nltk\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data set, shape: (50000, 1)\n",
      "content    0\n",
      "dtype: int64\n",
      "                                             content\n",
      "0  WASHINGTON  —   Congressional Republicans have...\n",
      "1  After the bullet shells get counted, the blood...\n",
      "2  When Walt Disney’s “Bambi” opened in 1942, cri...\n",
      "3  Death may be the great equalizer, but it isn’t...\n",
      "4  SEOUL, South Korea  —   North Korea’s leader, ...\n",
      "5  LONDON  —   Queen Elizabeth II, who has been b...\n",
      "6  BEIJING  —   President Tsai   of Taiwan sharpl...\n",
      "7  Danny Cahill stood, slightly dazed, in a blizz...\n",
      "8  Just how   is Hillary Kerr, the    founder of ...\n",
      "9  Angels are everywhere in the Muñiz family’s ap...\n"
     ]
    }
   ],
   "source": [
    "# Large dataset model:\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # read in data\n",
    "# df_1 = pd.read_csv('Data/articles1.csv')['content'].to_frame()  # only get content-column\n",
    "# df_2 = pd.read_csv('Data/articles2.csv')['content'].to_frame()\n",
    "# df_3 = pd.read_csv('Data/articles3.csv')['content'].to_frame()\n",
    "# df = df_1.append(df_2).append(df_3)\n",
    "# print('\\nData set, shape:', df.shape)\n",
    "# print(df.head(5))\n",
    "\n",
    "# # check for missing data\n",
    "# print(df.isna().sum())  # shows no null values in content-column\n",
    "\n",
    "# # split data into ~67% training and ~33% testing\n",
    "# train, test = train_test_split(df, test_size=0.33, random_state=1)\n",
    "# print('\\nTraining data set, shape:', train.shape)\n",
    "# print('Testing data set, shape:', test.shape)\n",
    "\n",
    "# # reset indices\n",
    "# train = train.reset_index(drop=True)\n",
    "# test = test.reset_index(drop=True)\n",
    "# print(train.head(5))\n",
    "# print('\\n', test.head(5))\n",
    "\n",
    "# Small dataset model:\n",
    "# read in data\n",
    "df = pd.read_csv('Data/articles1.csv')['content'].to_frame()\n",
    "# df.drop(df.index[0:49999],0,inplace=True)  # drop a few rows to make dataset smaller and more manageable\n",
    "print('\\nData set, shape:', df.shape)\n",
    "# check for missing data\n",
    "print(df.isna().sum())  # shows no null values in content-column\n",
    "\n",
    "# pd.set_option('display.max_colwidth', None)\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load spacy nlp pre-processing pipeline to use for lemmatization\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create filter for gensim nlp pre-processing pipeline to include all steps except stemmatization\n",
    "CUSTOM_FILTERS = [lambda x: x.lower(),  # lowercase\n",
    "                  strip_tags,\n",
    "                  strip_punctuation,  # replace punctuation with whitespace\n",
    "                  strip_multiple_whitespaces,  # remove repeating whitespaces\n",
    "                  strip_numeric,  # remove numbers\n",
    "                  remove_stopwords,  # remove stopwords\n",
    "                  strip_short,  # remove words with less than 3 characters\n",
    "                  #  stem_text  # return porter-stemmed text,\n",
    "                 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, my name is something you'll never guess, Kim! ...But I wrote my signature. Right! My parents called me this, what can I say?\n"
     ]
    }
   ],
   "source": [
    "sample = \"Hello, my name is something you'll never guess, Kim! ...But I wrote my signature. Right! My parents called me this, what can I say?\"\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'guess', 'kim', 'wrote', 'signatur', 'right', 'parent', 'call']\n"
     ]
    }
   ],
   "source": [
    "# test sample string without filtered pipeline, i.e., with stemmatizer\n",
    "test_a = preprocess_string(sample)\n",
    "print(test_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'guess', 'kim', 'write', 'signature', 'right', 'parent', 'call']\n"
     ]
    }
   ],
   "source": [
    "# test sample string with filtered pipeline and lemmatizer\n",
    "test_b = ' '.join(preprocess_string(sample, CUSTOM_FILTERS))  # pre-process without stemmatizing\n",
    "lem = [token.lemma_ for token in nlp(test_b)]  # lemmatize\n",
    "print(lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_articles(x):\n",
    "    prep = ' '.join(preprocess_string(x, CUSTOM_FILTERS))\n",
    "    return [token.lemma_ for token in nlp(prep)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply final pipeline to all data\n",
    "df['preprocessed'] = df['content'].apply(preprocess_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print head of preprocessed df\n",
    "print(df['preprocessed'].head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('stopwords')\n",
    "r = Rake()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r.extract_keywords_from_text(df['content'].iloc[0])\n",
    "# r.get_ranked_phrases_with_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r.extract_keywords_from_text(df['content'].iloc[1])\n",
    "# r.get_ranked_phrases_with_scores()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
